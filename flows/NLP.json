{
  "id": "c8e3f9a5-c967-47be-af82-3c15db514693",
  "name": "NLP Flow",
  "flow": {
    "edges": [
      {
        "id": "reactflow__edge-a0645439-5569-4f1b-a41b-4a48deca5e77source_a0645439-5569-4f1b-a41b-4a48deca5e77_object-2a7d0684-3e92-48cb-afda-99d8a48cbc28target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any",
        "source": "a0645439-5569-4f1b-a41b-4a48deca5e77",
        "target": "2a7d0684-3e92-48cb-afda-99d8a48cbc28",
        "sourceHandle": "source_a0645439-5569-4f1b-a41b-4a48deca5e77_object",
        "targetHandle": "target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any"
      },
      {
        "id": "reactflow__edge-d6784351-b931-40a3-ab5e-9663b7a156fdsource_d6784351-b931-40a3-ab5e-9663b7a156fd_object-2a7d0684-3e92-48cb-afda-99d8a48cbc28target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any",
        "source": "d6784351-b931-40a3-ab5e-9663b7a156fd",
        "target": "2a7d0684-3e92-48cb-afda-99d8a48cbc28",
        "sourceHandle": "source_d6784351-b931-40a3-ab5e-9663b7a156fd_object",
        "targetHandle": "target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any"
      },
      {
        "id": "reactflow__edge-c0639286-a0c3-4212-9fe5-8ac2df909c31source_c0639286-a0c3-4212-9fe5-8ac2df909c31_object-2a7d0684-3e92-48cb-afda-99d8a48cbc28target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any",
        "source": "c0639286-a0c3-4212-9fe5-8ac2df909c31",
        "target": "2a7d0684-3e92-48cb-afda-99d8a48cbc28",
        "sourceHandle": "source_c0639286-a0c3-4212-9fe5-8ac2df909c31_object",
        "targetHandle": "target_2a7d0684-3e92-48cb-afda-99d8a48cbc28_any"
      }
    ],
    "nodes": [
      {
        "id": "a0645439-5569-4f1b-a41b-4a48deca5e77",
        "data": {
          "name": "constant_node",
          "description": "Describe the task of node python_node_0",
          "python_code": "def exec(myinput):\n  params = {\n    \"database_code\": \"alpdev_pg\",\n    \"schema_name\": \"cdmdefault\",\n    \"note_table\": \"note\",\n    \"note_nlp_table\": \"note_nlp\",\n    \"use_cache_db\": False\n  }\n  return params\n"
        },
        "type": "python_node",
        "width": 350,
        "height": 210,
        "dragging": false,
        "position": { "x": -530, "y": -550 },
        "selected": false,
        "dragHandle": "",
        "sourcePosition": "right",
        "targetPosition": "left",
        "positionAbsolute": { "x": -530, "y": -550 }
      },
      {
        "id": "d6784351-b931-40a3-ab5e-9663b7a156fd",
        "data": {
          "name": "nel_node",
          "description": "Describe the task of node python_node_1",
          "python_code": "from scispacy.linking import EntityLinker\nfrom scispacy.abbreviation import AbbreviationDetector\n\nclass EntityExtractorLinker(object):\n  def __init__(self, mapper) -> None:\n    self.pipelines= list()\n    self.mapper = mapper\n\n  def add_pipeline(self, model_name:str, linker_name:str):\n    logger = get_run_logger()\n    logger.info(f\"Adding pipeline for model '{model_name}' and linker '{linker_name}'\")\n    logger.info(f\"Loading model ...\")\n    nlp = spacy.load(model_name) \n    logger.info(f\"Adding pipe ...\")\n    nlp.add_pipe(\"abbreviation_detector\")\n    logger.info(\"Loading linker ...\")\n    nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": linker_name})\n    logger.info(\"Adding pipeline done\")\n    self.pipelines.append((model_name, linker_name, nlp))\n\n  def extract_entities(self, text:str, confidence_threshold:float=0.8):\n    logger = get_run_logger()\n    if len(self.pipelines) == 0:\n      logger.info(\"No NLP pipeline defined - use 'add_pipeline' before calling 'extract_entities'!\")\n      return None\n      \n    data = dict()    \n    for model_name, linker_name, nlp in self.pipelines:\n      linker = nlp.get_pipe(\"scispacy_linker\")\n      doc = nlp(text)\n      logger.info(f\"Found {len(doc.ents)} entities.\")\n\n      for entity in doc.ents:\n        # list of matches in knowledge base (e.g. in case of UMLS, list of (cui_code, match_probability) tuples):\n        kb_matches = entity._.kb_ents\n        if len(kb_matches)==0:\n          logger.info(f\"No knowledge base mapping found for entity '{entity}'. Skipping.\")\n          continue\n\n        code, confidence = kb_matches[0]\n        if confidence < confidence_threshold:\n          logger.info(f\"Confidence below threshold ({confidence} < {confidence_threshold}) for entity '{entity}'. Skipping.\")\n          continue\n        \n        #convert CUI codes to rxNorm or SNOMED\n        mappings = self.mapper.get_codes(code)\n        if not any(key in mappings for key in [\"RxNorm\",\"SNOMED\"]):\n          logger.info(\"No mapping found for UMLS CUI '{code}' to either RxNorm or SNOMED. Skipping.\")\n          continue\n\n        omop_code, vocabulary = (mappings.get(\"RxNorm\"), \"RxNorm\") if \"RxNorm\" in mappings else (mappings.get(\"SNOMED\"), \"SNOMED\")               \n        data.setdefault(\"raw_text\", list()).append(text[entity.start_char:entity.end_char])\n        data.setdefault(\"start\", list()).append(entity.start_char)\n        data.setdefault(\"end\", list()).append(entity.end_char)\n        data.setdefault(\"label\", list()).append(entity.label_)\n        data.setdefault(\"model\", list()).append(model_name)\n        data.setdefault(\"linker\", list()).append(linker_name)\n\n        kb_entity = linker.kb.cui_to_entity[code]\n        data.setdefault(\"concept_id\", list()).append(omop_code)\n        data.setdefault(\"vocabulary\", list()).append(vocabulary)\n        data.setdefault(\"confidence\", list()).append(confidence)\n        data.setdefault(\"UMLS_canonical_name\", list()).append(kb_entity.canonical_name)\n        data.setdefault(\"UMLS_definition\", list()).append(kb_entity.definition)\n\n    return pd.DataFrame(data)\n\ndef exec(myinput):\n  return EntityExtractorLinker"
        },
        "type": "python_node",
        "width": 350,
        "height": 210,
        "dragging": false,
        "position": { "x": -530, "y": -280 },
        "selected": false,
        "dragHandle": "",
        "sourcePosition": "right",
        "targetPosition": "left",
        "positionAbsolute": { "x": -530, "y": -280 }
      },
      {
        "id": "c0639286-a0c3-4212-9fe5-8ac2df909c31",
        "data": {
          "name": "umls2omop_node",
          "description": "Describe the task of node python_node_2",
          "python_code": "from typing import List, Union\n\nCUItoOHDSI_CSV=f\"flows/ner_extract_plugin/external/CUItoOHDSIv1.csv\"\n\nclass CIO2OMOP(object):\n    def __init__(self, mapping_csv: str) -> None:\n        with open(mapping_csv, 'r') as in_mappings:\n            self.mappings_df = pd.read_csv(in_mappings, index_col=0)\n            self.coding_systems = list(self.mappings_df[\"vocabulary_id\"].unique())\n\n    def get_supported_vocabulaires(self) -> List[str]:\n        return self.coding_systems\n\n    def get_codes(self, CUI:str):\n        try:\n            results = self.mappings_df.loc[[CUI]]\n        except KeyError:\n            # CUI code not found\n            return dict()\n        \n        return dict([(vocabulary_id, (results.loc[results[\"vocabulary_id\"]==vocabulary_id][\"concept_id\"]).squeeze()) \n                     for vocabulary_id in results[\"vocabulary_id\"]])\n\n    def get_mappings(self, CUI:Union[str,List[str]]):\n        return self.mappings_df.loc[CUI]\n\ndef exec(myinput):\n  mapper = CIO2OMOP(mapping_csv=CUItoOHDSI_CSV)\n  return mapper"
        },
        "type": "python_node",
        "width": 350,
        "height": 210,
        "dragging": false,
        "position": { "x": -530, "y": -20 },
        "selected": false,
        "dragHandle": "",
        "sourcePosition": "right",
        "targetPosition": "left",
        "positionAbsolute": { "x": -530, "y": -20 }
      },
      {
        "id": "2a7d0684-3e92-48cb-afda-99d8a48cbc28",
        "data": {
          "name": "flow_node",
          "description": "Describe the task of node python_node_3",
          "python_code": "from _shared_flow_utils.dao.DBDao import DBDao\nfrom datetime import datetime\nimport os\nfrom prefect import flow\n\n@flow(log_prints=True)\ndef exec(myinput):\n  logger = get_run_logger()\n  model_info = spacy.info()['pipelines']\n  logger.info(f\"The following spacy models are available: {model_info}\")\n  logger.info(\"Start the connection to database\")\n\n  dbdao = DBDao(use_cache_db=False, database_code=database_code)\n  entity_extractor_linker = myinput.get(\"nel_node\").result\n  mapper = myinput.get(\"umls2omop_node\").result\n\n  with dbdao.ibis_connect() as conn:\n    logger.info(\"Loading Notes\")\n    note = conn.table(note_table, database=schema_name)\n    note_nlp = conn.table(note_nlp_table, database=schema_name)\n    record = note.select(['note_id','note_text']).execute()\n    count = note_nlp.count().execute()\n    rst_df = pd.DataFrame()\n\n    for note_id, note_text in record.values:\n      # Two steps of add_pipeline and extract\n      logger.info(f\"Start to analyze note {note_id}\")\n      medical_ner_nel = entity_extractor_linker(mapper)\n      medical_ner_nel.add_pipeline(model_name=\"en_ner_bc5cdr_md\", linker_name=\"umls\")\n      df1 = medical_ner_nel.extract_entities(text=note_text, confidence_threshold=0.8)\n\n      medical_ner_nel = entity_extractor_linker(mapper)\n      medical_ner_nel.add_pipeline(model_name=\"en_core_med7_trf\", linker_name=\"rxnorm\")\n      df2 = medical_ner_nel.extract_entities(text=note_text, confidence_threshold=0.8)\n      note_df = pd.concat([df1,df2]).reset_index(drop=True)\n\n      # logger.info(f\"Complete the analysis of note {note_id}\")\n      # map note_df to note_nlp table\n      note_df['note_id'] = note_id\n      note_df['section_concept_id'] = -1\n      note_df['snippet'] = note_df.apply(lambda x: note_text[x['start']-10:x['end']+10], axis=1)\n      note_df['note_nlp_source_concept_id'] = -1\n      note_df['nlp_system'] = note_df.apply(lambda x: '-'.join(x[['model','linker']]+[f'-{model_info[x.model]}']), axis=1)\n      note_df['nlp_date'] = datetime.now().strftime(\"%Y-%m-%d\")\n      note_df['nlp_datetime'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n      note_df['term_exists'] = 'U'\n      note_df['term_temporal'] ='N/A'\n      note_df['term_modifiers'] ='N/A'\n      note_df = note_df.rename(columns={'start':'offset',\n                                      'raw_text':'lexical_variant',\n                                      'concept_id':'note_nlp_concept_id',\n                                  })\n      note_df['note_nlp_id'] = note_df.index.values + 1 + count\n      \n      rst_df = pd.concat([rst_df,note_df]).reset_index(drop=True)\n      count += len(note_df)\n      logger.info(f\"Results of note_id: {note_id} for confidence_threshold=0.8 done\")\n  \n      cols = ['note_nlp_id',\n              'note_id',\n              'section_concept_id',\n              'snippet',\n              'offset',\n              'lexical_variant',\n              'note_nlp_concept_id',\n              'note_nlp_source_concept_id',\n              'nlp_system',\n              'nlp_date',\n              'nlp_datetime',\n              'term_exists',\n              'term_temporal',\n              'term_modifiers']\n  with dbdao.engine.connect() as conn:\n    rst_df[cols].to_sql(name = note_nlp_table,\n                        con = conn,\n                        schema = schema_name,\n                        if_exists = 'append',\n                        index = False,\n                        chunksize = 32,\n                )"
        },
        "type": "python_node",
        "width": 350,
        "height": 210,
        "dragging": false,
        "position": { "x": -10, "y": -280 },
        "selected": true,
        "dragHandle": "",
        "sourcePosition": "right",
        "targetPosition": "left",
        "positionAbsolute": { "x": -10, "y": -280 }
      }
    ],
    "variables": [
      { "key": "database_code", "value": "alpdev_pg" },
      { "key": "schema_name", "value": "cdmdefault" },
      { "key": "note_table", "value": "note" },
      { "key": "note_nlp_table", "value": "note_nlp" }
    ],
    "importLibs": [
      "from prefect.logging import get_run_logger",
      "import pandas as pd",
      "import spacy"
    ]
  },
  "createdBy": "D4L",
  "createdDate": "1900-01-01T00:00:00.000Z"
}
